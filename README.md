# ğŸ”„ Ollama â†” LM Studio Proxy Server

**A powerful Rust-based proxy server that seamlessly bridges Ollama API and LM Studio, enabling bidirectional
communication and universal compatibility.**

---

## âœ¨ What This Does

Transform your LM Studio setup into a **universal AI backend** that works with:

- ğŸ¦™ **Ollama clients** (with full API translation)
- ğŸ¤– **OpenAI-compatible tools** (VS Code, GitHub Copilot, etc.)
- ğŸ› ï¸ **Any HTTP client** expecting either API format

## ğŸš€ Key Features

### ğŸ¯ **Dual API Support**

- **Ollama API** (`/api/*`) - Full translation to LM Studio format
- **OpenAI API** (`/v1/*`) - Direct passthrough to LM Studio
- **Smart Routing** - Automatically detects and handles both formats

### ğŸ§  **Intelligent Model Management**

- **Auto-Retry Logic** - Automatically loads models when needed
- **Smart Name Mapping** - Handles model name variations (`model:2` â†’ `model`)
- **Format Translation** - Seamless conversion between API formats

### ğŸ¨ **Enhanced Responses**

- **Reasoning Integration** - Merges LM Studio's reasoning content
- **Timing Estimates** - Provides Ollama-compatible performance metrics
- **Token Counting** - Accurate usage statistics
- **Error Handling** - Graceful degradation with helpful messages

### âš¡ **Performance & Reliability**

- **Built in Rust** - Memory-safe, fast, and reliable
- **Concurrent Handling** - Multiple requests simultaneously
- **Comprehensive Logging** - Detailed request/response tracking
- **Configurable Timeouts** - Customizable retry behavior

---

## ğŸª **Supported Endpoints**

| **Ollama API**       | **LM Studio Equivalent**    | **Status**            |
|----------------------|-----------------------------|-----------------------|
| `GET /api/tags`      | `GET /v1/models`            | âœ… **Full Support**    |
| `POST /api/chat`     | `POST /v1/chat/completions` | âœ… **Full Support**    |
| `POST /api/generate` | `POST /v1/completions`      | âœ… **Full Support**    |
| `POST /api/embed`    | `POST /v1/embeddings`       | âœ… **Full Support**    |
| `GET /api/ps`        | *(simulated)*               | âœ… **Simulated**       |
| `POST /api/show`     | *(simulated)*               | âœ… **Simulated**       |
| `GET /api/version`   | *(hardcoded)*               | âœ… **Static Response** |

| **OpenAI/LM Studio API**    | **Handling**              |
|-----------------------------|---------------------------|
| `GET /v1/models`            | ğŸ”„ **Direct Passthrough** |
| `POST /v1/chat/completions` | ğŸ”„ **Direct Passthrough** |
| `POST /v1/completions`      | ğŸ”„ **Direct Passthrough** |
| `POST /v1/embeddings`       | ğŸ”„ **Direct Passthrough** |

---

## ğŸƒâ€â™‚ï¸ **Quick Start**

### **1. Prerequisites**

- ğŸ¦€ Rust 1.70+ installed
- ğŸ§  LM Studio running with a model loaded
- ğŸŒ Network access between components

### **2. Installation**

```bash
# Clone and build
git clone <your-repo-url>
cd ollama-lmstudio-proxy
cargo build --release

# Run with defaults
./target/release/ollama-lmstudio-proxy
```

### **3. Configuration**

```bash
# Custom configuration
./target/release/ollama-lmstudio-proxy \
  --listen 0.0.0.0:11434 \
  --lmstudio-url http://localhost:1234 \
  --load-timeout-seconds 10

# Disable logging
./target/release/ollama-lmstudio-proxy --no-log

# Help
./target/release/ollama-lmstudio-proxy --help
```

---

## ğŸ§ª **Usage Examples**

### **With Ollama Clients**

```bash
# List models (Ollama format)
curl http://localhost:11434/api/tags

# Chat completion (Ollama format)
curl http://localhost:11434/api/chat -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Explain quantum computing"}
  ]
}'

# Text generation (Ollama format)
curl http://localhost:11434/api/generate -d '{
  "model": "your-model-name",
  "prompt": "The future of AI is"
}'
```

### **With OpenAI/VS Code**

```bash
# List models (OpenAI format)
curl http://localhost:11434/v1/models

# Chat completion (OpenAI format)
curl http://localhost:11434/v1/chat/completions -d '{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Write a Python function"}
  ],
  "temperature": 0.7,
  "max_tokens": 1000
}'
```

### **VS Code Integration**

1. Install an OpenAI-compatible extension
2. Set API endpoint to: `http://localhost:11434`
3. Use any model name from your LM Studio
4. Start coding with AI assistance! ğŸ‰

---

## ğŸ›ï¸ **Configuration Options**

| **Flag**                 | **Default**             | **Description**                  |
|--------------------------|-------------------------|----------------------------------|
| `--listen`               | `0.0.0.0:11434`         | Address and port to listen on    |
| `--lmstudio-url`         | `http://localhost:1234` | LM Studio API endpoint           |
| `--no-log`               | `false`                 | Disable request/response logging |
| `--load-timeout-seconds` | `5`                     | Model loading retry timeout      |

---

## ğŸ”§ **Troubleshooting**

### **Common Issues**

**ğŸ”Œ "Connection Refused"**

- Ensure LM Studio is running and accessible
- Check firewall settings
- Verify the `--lmstudio-url` parameter

**ğŸ¤– "Model Not Found"**

- Load a model in LM Studio first
- Check available models: `curl http://localhost:11434/api/tags`
- Verify model names match between tools

**ğŸ“ "VS Code Not Working"**

- Ensure you're using `/v1/*` endpoints in VS Code settings
- Check that the API endpoint is set to `http://localhost:11434`
- Verify the model name exists in LM Studio

**â±ï¸ "Slow Responses"**

- Increase `--load-timeout-seconds` for slow model loading
- Check LM Studio performance and hardware resources
- Monitor logs for retry attempts

---

## ğŸŒŸ **Why This Proxy?**

### **ğŸ¯ Universal Compatibility**

- Works with **any Ollama client** without modification
- Compatible with **VS Code, GitHub Copilot, and OpenAI tools**
- **Single endpoint** for multiple API formats

### **ğŸ§  Smart Translation**

- **Preserves reasoning** from advanced models
- **Accurate timing** and token counting
- **Proper error handling** with helpful messages

### **âš¡ Performance**

- **Rust-powered** for maximum speed and safety
- **Automatic retries** for seamless experience
- **Concurrent processing** for multiple clients

### **ğŸ› ï¸ Developer Friendly**

- **Comprehensive logging** for debugging
- **Flexible configuration** for any setup
- **Open source** and easily extendable

---

## ğŸ“Š **Performance**

- **ğŸš€ Low Latency**: Minimal overhead between client and LM Studio
- **ğŸ”„ Auto-Recovery**: Intelligent retry logic for model loading
- **ğŸ“ˆ Scalable**: Handles multiple concurrent connections
- **ğŸ’¾ Memory Efficient**: Rust's zero-cost abstractions

---

## ğŸ¤ **Contributing**

We welcome contributions! Whether it's:

- ğŸ› Bug fixes
- âœ¨ New features
- ğŸ“š Documentation improvements
- ğŸ§ª Test coverage

---

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE]() file for details.

---

## ğŸ™ **Acknowledgments**

- **[Ollama](https://ollama.ai/)** - For the excellent API design
- **[LM Studio](https://lmstudio.ai/)** - For the powerful local LLM platform
- **[Rust Community](https://www.rust-lang.org/)** - For the amazing ecosystem
- **[Anthropic](https://www.anthropic.com/)** - For Claude's development assistance

---

**ğŸ‰ Transform your LM Studio into a universal AI backend today!**

[â­ Star this repo](https://github.com/your-username/ollama-lmstudio-proxy) â€¢ [ğŸ› Report Bug](https://github.com/your-username/ollama-lmstudio-proxy/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/your-username/ollama-lmstudio-proxy/issues)
